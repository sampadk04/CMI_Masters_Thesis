% Maestro V2
@inproceedings{maestrov2,
  title={Enabling Factorized Piano Music Modeling and Generation with the {MAESTRO} Dataset},
  author={Curtis Hawthorne and Andriy Stasyuk and Adam Roberts and Ian Simon and Cheng-Zhi Anna Huang and Sander Dieleman and Erich Elsen and Jesse Engel and Douglas Eck},
  booktitle={International Conference on Learning Representations},
  version={2},
  year={2019},
  url={https://openreview.net/forum?id=r1lYRjC9F7},
}

% Tiny Shakespeare
@misc{tinyshakespeare,
  author={Karpathy, Andrej},
  title={char-rnn},
  year={2015},
  howpublished={\url{https://github.com/karpathy/char-rnn} }
}

% MNIST
@article{mnist,
  added-at = {2021-10-12T06:56:46.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/andolab},
  description = {Nur für Referenzzwecke verwendet (MNIST)},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {Image_Classification_Benchmark},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2023-01-31T20:34:07.000+0100},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

% Pile
@misc{pileds,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Papers

% Glorot Initialization
@article{glorotinit,
author = {Glorot, Xavier and Bengio, Y.},
year = {2010},
month = {01},
pages = {249-256},
title = {Understanding the difficulty of training deep feedforward neural networks},
volume = {9},
journal = {Journal of Machine Learning Research - Proceedings Track}
}

% Transformer
@misc{transformers,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% MQA
@misc{mqa,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

% GPT2
@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

% LLaMA
@misc{llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% LLaMA2
@misc{llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Flash Attention
@misc{flashattn,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% HIPPO
@misc{hippo,
      title={HiPPO: Recurrent Memory with Optimal Polynomial Projections}, 
      author={Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Re},
      year={2020},
      eprint={2008.07669},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% S4
@misc{s4,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Diagonal SSM

@misc{dssm,
      title={Diagonal State Spaces are as Effective as Structured State Spaces}, 
      author={Ankit Gupta and Albert Gu and Jonathan Berant},
      year={2022},
      eprint={2203.14343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% H3
@misc{h3,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Mamba
@misc{mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2023},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Universality of Linear RNN
@misc{universality_lrnn,
      title={Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues}, 
      author={Antonio Orvieto and Soham De and Caglar Gulcehre and Razvan Pascanu and Samuel L. Smith},
      year={2024},
      eprint={2307.11888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% LRU
@misc{lru,
      title={Resurrecting Recurrent Neural Networks for Long Sequences}, 
      author={Antonio Orvieto and Samuel L Smith and Albert Gu and Anushan Fernando and Caglar Gulcehre and Razvan Pascanu and Soham De},
      year={2023},
      eprint={2303.06349},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Hawk and Griffin
@misc{hawkgriffin,
      title={Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models}, 
      author={Soham De and Samuel L. Smith and Anushan Fernando and Aleksandar Botev and George Cristian-Muraru and Albert Gu and Ruba Haroun and Leonard Berrada and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and Arnaud Doucet and David Budden and Yee Whye Teh and Razvan Pascanu and Nando De Freitas and Caglar Gulcehre},
      year={2024},
      eprint={2402.19427},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Benchmarks

% LRA
@misc{lra,
      title={Long Range Arena: A Benchmark for Efficient Transformers}, 
      author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
      year={2020},
      eprint={2011.04006},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}