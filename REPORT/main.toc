\contentsline {chapter}{\numberline {1}Introduction to Deep Sequence Modelling}{3}{chapter.1}%
\contentsline {section}{\numberline {1.1}Deep Sequence Models}{3}{section.1.1}%
\contentsline {section}{\numberline {1.2}Long Range Arena}{5}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Pathfinder and PathX}{5}{subsection.1.2.1}%
\contentsline {chapter}{\numberline {2}Transformers and the Attention Mechanism}{7}{chapter.2}%
\contentsline {section}{\numberline {2.1}Transformers and Attention}{7}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Encoder Block}{7}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Decoder Block}{7}{subsection.2.1.2}%
\contentsline {section}{\numberline {2.2}Attention Mechanism}{8}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Self-Attention}{8}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Scaled Dot-Product Attention}{9}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Multi-Head Attention}{9}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Limitations of Attention}{9}{section.2.3}%
\contentsline {chapter}{\numberline {3}SSMs: Foundations and the S4 Architecture}{11}{chapter.3}%
\contentsline {section}{\numberline {3.1}State Space Models: A Formal Introduction}{11}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Continuous-Time SSMs}{12}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Linear Time-Invariant SSMs}{12}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Discrete-Time SSMs}{12}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Euler Method}{13}{subsection.3.1.4}%
\contentsline {subsection}{\numberline {3.1.5}Bilinear Transform}{13}{subsection.3.1.5}%
\contentsline {subsection}{\numberline {3.1.6}Zero-Order Hold (ZoH)}{13}{subsection.3.1.6}%
\contentsline {section}{\numberline {3.2}S4}{14}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Training S4: Convolutional Representation}{14}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Challenges with Naive Implementation}{15}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Adding Further Structure to A}{15}{subsection.3.2.3}%
\contentsline {subsubsection}{Using HIPPO Matrix}{15}{subsection.3.2.3}%
\contentsline {subsubsection}{Complexity Improvements}{15}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}Architecture of Deep S4}{16}{subsection.3.2.4}%
\contentsline {subsection}{\numberline {3.2.5}Empirical Performance in LRA}{17}{subsection.3.2.5}%
\contentsline {subsection}{\numberline {3.2.6}Limitations of S4 and LTIs in Language Modelling}{17}{subsection.3.2.6}%
\contentsline {chapter}{\numberline {4}Selective SSM and Mamba}{19}{chapter.4}%
\contentsline {section}{\numberline {4.1}S6}{19}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Discretization Strategy: ZoH}{19}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Breaking LTI: Adding Selectivity}{20}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Adding Structure to A}{20}{subsection.4.1.3}%
\contentsline {section}{\numberline {4.2}Mamba}{22}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Architecture}{22}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Empirical Results}{22}{subsection.4.2.2}%
\contentsline {subsubsection}{Scaling Laws}{23}{subsection.4.2.2}%
\contentsline {subsubsection}{Efficiency Benchmarks}{23}{figure.caption.13}%
\contentsline {chapter}{\numberline {5}Linear Recurrent Units: Simplicity and Universality}{24}{chapter.5}%
\contentsline {section}{\numberline {5.1}Universality of Linear RNN followed by Non-Linear Projections}{24}{section.5.1}%
\contentsline {section}{\numberline {5.2}LRU}{25}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Linear Recurrences}{25}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Complex Diagonal Recurrent Matrices}{26}{subsection.5.2.2}%
\contentsline {subsubsection}{Switching to Diagonal Matrices}{26}{subsection.5.2.2}%
\contentsline {subsubsection}{Controlling Stability with Eigenvalues}{27}{subsection.5.2.2}%
\contentsline {subsubsection}{Glorot Equivalent Initialization}{28}{subsection.5.2.2}%
\contentsline {subsubsection}{Stable Exponential Parameterization}{28}{subsection.5.2.2}%
\contentsline {paragraph}{Optimization under Exponential Parameterization}{28}{subsection.5.2.2}%
\contentsline {subsubsection}{Normalization}{29}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Deep LRU Architecture}{29}{subsection.5.2.3}%
\contentsline {subsection}{\numberline {5.2.4}Empirical Results on LRA}{29}{subsection.5.2.4}%
\contentsline {chapter}{\numberline {6}Hybrid Models: Hawk and Griffin}{31}{chapter.6}%
\contentsline {section}{\numberline {6.1}Model Architecture with Residual Blocks}{31}{section.6.1}%
\contentsline {section}{\numberline {6.2}RG-LRU: A Modification of LRU}{32}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Gating mechanism of RG-LRU}{32}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Temporal Conv 1D Layer}{32}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Local Sliding Window Attention}{33}{section.6.3}%
\contentsline {section}{\numberline {6.4}Hawk}{33}{section.6.4}%
\contentsline {section}{\numberline {6.5}Griffin}{33}{section.6.5}%
\contentsline {section}{\numberline {6.6}Optimisations}{33}{section.6.6}%
\contentsline {section}{\numberline {6.7}Empirical Results}{33}{section.6.7}%
\contentsline {subsection}{\numberline {6.7.1}Scaling Laws and Throughput}{33}{subsection.6.7.1}%
\contentsline {subsection}{\numberline {6.7.2}Character Normalised Accuracy}{34}{subsection.6.7.2}%
\contentsline {chapter}{\numberline {7}Experiments}{35}{chapter.7}%
\contentsline {section}{\numberline {7.1}Tiny Shakespeare}{35}{section.7.1}%
\contentsline {section}{\numberline {7.2}Maestro V2}{36}{section.7.2}%
\contentsline {section}{\numberline {7.3}Sequential MNIST}{36}{section.7.3}%
