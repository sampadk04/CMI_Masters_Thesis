\relax 
\abx@aux@refcontext{nty/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{0}{transformers}
\abx@aux@segm{0}{0}{transformers}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{lru}
\abx@aux@segm{0}{0}{lru}
\abx@aux@cite{0}{hawkgriffin}
\abx@aux@segm{0}{0}{hawkgriffin}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to Deep Sequence Modelling}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Deep Sequence Models}{3}{section.1.1}\protected@file@percent }
\abx@aux@cite{0}{transformers}
\abx@aux@segm{0}{0}{transformers}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{lru}
\abx@aux@segm{0}{0}{lru}
\abx@aux@cite{0}{hawkgriffin}
\abx@aux@segm{0}{0}{hawkgriffin}
\newlabel{SC@1}{{\caption@xref {??}{ on input line 135}}{4}{Deep Sequence Models}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A deep sequence model is a neural network architecture built around a core sequence transformation such as convolutions, attention, or recurrence, and comprising additional components such as normalisation layers, linear layers, and residual connections. This boxed architecture block is usually composed repeatedly into a deep neural network.\relax }}{4}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{deep-sequence-arch}{{1.1}{4}{A deep sequence model is a neural network architecture built around a core sequence transformation such as convolutions, attention, or recurrence, and comprising additional components such as normalisation layers, linear layers, and residual connections. This boxed architecture block is usually composed repeatedly into a deep neural network.\relax }{figure.caption.5}{}}
\abx@aux@cite{0}{lra}
\abx@aux@segm{0}{0}{lra}
\abx@aux@cite{0}{lra}
\abx@aux@segm{0}{0}{lra}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Long Range Arena}{5}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Pathfinder and PathX}{5}{subsection.1.2.1}\protected@file@percent }
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{transformers}
\abx@aux@segm{0}{0}{transformers}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Samples of Pathfinder task.\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{pathfinder-task}{{1.2}{6}{Samples of Pathfinder task.\relax }{figure.caption.6}{}}
\abx@aux@cite{0}{transformers}
\abx@aux@segm{0}{0}{transformers}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Transformers and the Attention Mechanism}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Transformers and Attention}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Encoder Block}{7}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Decoder Block}{7}{subsection.2.1.2}\protected@file@percent }
\abx@aux@cite{0}{transformers}
\abx@aux@segm{0}{0}{transformers}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Attention Mechanism}{8}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Self-Attention}{8}{subsection.2.2.1}\protected@file@percent }
\abx@aux@cite{0}{transformers}
\abx@aux@segm{0}{0}{transformers}
\abx@aux@cite{0}{transformers}
\abx@aux@segm{0}{0}{transformers}
\abx@aux@cite{0}{mqa}
\abx@aux@segm{0}{0}{mqa}
\abx@aux@cite{0}{lra}
\abx@aux@segm{0}{0}{lra}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Scaled Dot-Product Attention}{9}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Multi-Head Attention}{9}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Limitations of Attention}{9}{section.2.3}\protected@file@percent }
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}SSMs: Foundations and the S4 Architecture}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}State Space Models: A Formal Introduction}{11}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Block diagram representation of Linear SSM\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{ssm_bd}{{3.1}{11}{Block diagram representation of Linear SSM\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Continuous-Time SSMs}{12}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Linear Time-Invariant SSMs}{12}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Discrete-Time SSMs}{12}{subsection.3.1.3}\protected@file@percent }
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Euler Method}{13}{subsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Bilinear Transform}{13}{subsection.3.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Zero-Order Hold (ZoH)}{13}{subsection.3.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}S4}{14}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Training S4: Convolutional Representation}{14}{subsection.3.2.1}\protected@file@percent }
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{hippo}
\abx@aux@segm{0}{0}{hippo}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Challenges with Naive Implementation}{15}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Adding Further Structure to A}{15}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Using HIPPO Matrix}{15}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Complexity Improvements}{15}{subsection.3.2.3}\protected@file@percent }
\abx@aux@cite{0}{lra}
\abx@aux@segm{0}{0}{lra}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Complexity of various sequence models in terms of sequence length ($L$), batch size ($B$), and hidden dimension ($H$); tildes denote log factors. Metrics are computation for 1 sample and time-step. For simplicity, the state size of S4 $N = H$. Convolutions are efficient for training while recurrence is efficient for inference, while SSMs combine the strengths of both.\relax }}{16}{table.caption.8}\protected@file@percent }
\newlabel{tab:complexity}{{3.1}{16}{Complexity of various sequence models in terms of sequence length ($L$), batch size ($B$), and hidden dimension ($H$); tildes denote log factors. Metrics are computation for 1 sample and time-step. For simplicity, the state size of S4 $N = H$. Convolutions are efficient for training while recurrence is efficient for inference, while SSMs combine the strengths of both.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Architecture of Deep S4}{16}{subsection.3.2.4}\protected@file@percent }
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Empirical Performance in LRA}{17}{subsection.3.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Original Transformer (Top) variants vs S4 (Bottom) in LRA\relax }}{17}{figure.caption.9}\protected@file@percent }
\newlabel{s4-lra}{{3.2}{17}{Original Transformer (Top) variants vs S4 (Bottom) in LRA\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Limitations of S4 and LTIs in Language Modelling}{17}{subsection.3.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by LTIs. (Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs.\relax }}{18}{figure.caption.10}\protected@file@percent }
\newlabel{selective-copy-induction-heads}{{3.3}{18}{The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by LTIs. (Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs.\relax }{figure.caption.10}{}}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Selective SSM and Mamba}{19}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}S6}{19}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Discretization Strategy: ZoH}{19}{subsection.4.1.1}\protected@file@percent }
\abx@aux@cite{0}{dssm}
\abx@aux@segm{0}{0}{dssm}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{hippo}
\abx@aux@segm{0}{0}{hippo}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Breaking LTI: Adding Selectivity}{20}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Adding Structure to A}{20}{subsection.4.1.3}\protected@file@percent }
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{flashattn}
\abx@aux@segm{0}{0}{flashattn}
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Memory Heirarchy with Bandwidth and Size\relax }}{21}{figure.caption.11}\protected@file@percent }
\newlabel{gpu-band}{{4.1}{21}{Memory Heirarchy with Bandwidth and Size\relax }{figure.caption.11}{}}
\abx@aux@cite{0}{pileds}
\abx@aux@segm{0}{0}{pileds}
\abx@aux@cite{0}{pileds}
\abx@aux@segm{0}{0}{pileds}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Mamba}{22}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Architecture}{22}{subsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces This represents a single Mamba Block, which can be stacked to form a deep Mamba model.\relax }}{22}{figure.caption.12}\protected@file@percent }
\newlabel{mamba-arch}{{4.2}{22}{This represents a single Mamba Block, which can be stacked to form a deep Mamba model.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Empirical Results}{22}{subsection.4.2.2}\protected@file@percent }
\abx@aux@cite{0}{llama}
\abx@aux@segm{0}{0}{llama}
\@writefile{toc}{\contentsline {subsubsection}{Scaling Laws}{23}{subsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Scaling Laws of models with sizes from $125M - 1.3B$ parameters, trained on Pile Dataset \blx@tocontentsinit {0}\cite {pileds}.\relax }}{23}{figure.caption.13}\protected@file@percent }
\newlabel{mamba-scale}{{4.3}{23}{Scaling Laws of models with sizes from $125M - 1.3B$ parameters, trained on Pile Dataset \cite {pileds}.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficiency Benchmarks}{23}{figure.caption.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces (Left) Training Times (Right) Inference Times of Mamba vs Transformers\relax }}{23}{figure.caption.14}\protected@file@percent }
\newlabel{mamba-eff}{{4.4}{23}{(Left) Training Times (Right) Inference Times of Mamba vs Transformers\relax }{figure.caption.14}{}}
\abx@aux@cite{0}{lru}
\abx@aux@segm{0}{0}{lru}
\abx@aux@cite{0}{lra}
\abx@aux@segm{0}{0}{lra}
\abx@aux@cite{0}{universality_lrnn}
\abx@aux@segm{0}{0}{universality_lrnn}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Linear Recurrent Units: Simplicity and Universality}{24}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Universality of Linear RNN followed by Non-Linear Projections}{24}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Example: Linear RNN + position-wise MLP for flattened MNIST digits reconstruction. The role of linear RNN is to compress and store the input sequence into the hidden states, from which we can recover past tokens using a MLP. As hidden size $N$ increases, the reconstruction becomes more and more faithful.\relax }}{24}{figure.caption.15}\protected@file@percent }
\newlabel{univ-rnn}{{5.1}{24}{Example: Linear RNN + position-wise MLP for flattened MNIST digits reconstruction. The role of linear RNN is to compress and store the input sequence into the hidden states, from which we can recover past tokens using a MLP. As hidden size $N$ increases, the reconstruction becomes more and more faithful.\relax }{figure.caption.15}{}}
\abx@aux@cite{0}{lru}
\abx@aux@segm{0}{0}{lru}
\abx@aux@cite{0}{lra}
\abx@aux@segm{0}{0}{lra}
\abx@aux@cite{0}{universality_lrnn}
\abx@aux@segm{0}{0}{universality_lrnn}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}LRU}{25}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Linear Recurrences}{25}{subsection.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Effect of removing the non-linearity from the recurrence on the feasible LRA tasks.\relax }}{25}{figure.caption.16}\protected@file@percent }
\newlabel{lrnn-perf}{{5.2}{25}{Effect of removing the non-linearity from the recurrence on the feasible LRA tasks.\relax }{figure.caption.16}{}}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{universality_lrnn}
\abx@aux@segm{0}{0}{universality_lrnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Complex Diagonal Recurrent Matrices}{26}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Switching to Diagonal Matrices}{26}{subsection.5.2.2}\protected@file@percent }
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{lru}
\abx@aux@segm{0}{0}{lru}
\abx@aux@cite{0}{glorotinit}
\abx@aux@segm{0}{0}{glorotinit}
\abx@aux@cite{0}{lru}
\abx@aux@segm{0}{0}{lru}
\@writefile{toc}{\contentsline {subsubsection}{Controlling Stability with Eigenvalues}{27}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Glorot Equivalent Initialization}{28}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stable Exponential Parameterization}{28}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimization under Exponential Parameterization}{28}{subsection.5.2.2}\protected@file@percent }
\abx@aux@cite{0}{lru}
\abx@aux@segm{0}{0}{lru}
\abx@aux@cite{0}{lra}
\abx@aux@segm{0}{0}{lra}
\abx@aux@cite{0}{s4}
\abx@aux@segm{0}{0}{s4}
\@writefile{toc}{\contentsline {subsubsection}{Normalization}{29}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Deep LRU Architecture}{29}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Empirical Results on LRA}{29}{subsection.5.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Effects of normalization on linear diagonal RNNs with stable exponential parameterization. Results showcase the advantage of taking initialization close to the unit circle under proper $\mathbf  {\gamma }$ normalization.\relax }}{30}{figure.caption.17}\protected@file@percent }
\newlabel{lru-perf}{{5.3}{30}{Effects of normalization on linear diagonal RNNs with stable exponential parameterization. Results showcase the advantage of taking initialization close to the unit circle under proper $\mathbf {\gamma }$ normalization.\relax }{figure.caption.17}{}}
\abx@aux@cite{0}{hawkgriffin}
\abx@aux@segm{0}{0}{hawkgriffin}
\abx@aux@cite{0}{transformers}
\abx@aux@segm{0}{0}{transformers}
\abx@aux@cite{0}{mqa}
\abx@aux@segm{0}{0}{mqa}
\abx@aux@cite{0}{mqa}
\abx@aux@segm{0}{0}{mqa}
\abx@aux@cite{0}{hawkgriffin}
\abx@aux@segm{0}{0}{hawkgriffin}
\abx@aux@cite{0}{llama}
\abx@aux@segm{0}{0}{llama}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Hybrid Models: Hawk and Griffin}{31}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Model Architecture with Residual Blocks}{31}{section.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces (a) The main backbone of Hawk and Griffin is the residual block, which is stacked $N$ times. (b) The gated MLP Block (c) The Recurrent Block containing the RG-LRU layer. For baseline comparison, this recurrent block is replaced with Multi-Query Attention (MQA) \blx@tocontentsinit {0}\cite {mqa}.\relax }}{31}{figure.caption.18}\protected@file@percent }
\newlabel{hg_resblock_arch}{{6.1}{31}{(a) The main backbone of Hawk and Griffin is the residual block, which is stacked $N$ times. (b) The gated MLP Block (c) The Recurrent Block containing the RG-LRU layer. For baseline comparison, this recurrent block is replaced with Multi-Query Attention (MQA) \cite {mqa}.\relax }{figure.caption.18}{}}
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{h3}
\abx@aux@segm{0}{0}{h3}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}RG-LRU: A Modification of LRU}{32}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Gating mechanism of RG-LRU}{32}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Temporal Conv 1D Layer}{32}{subsection.6.2.2}\protected@file@percent }
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Local Sliding Window Attention}{33}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Hawk}{33}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Griffin}{33}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Optimisations}{33}{section.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Empirical Results}{33}{section.6.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Scaling Laws and Throughput}{33}{subsection.6.7.1}\protected@file@percent }
\abx@aux@cite{0}{mamba}
\abx@aux@segm{0}{0}{mamba}
\abx@aux@cite{0}{llama2}
\abx@aux@segm{0}{0}{llama2}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Comparing validation loss and throughput of Hawk, Griffin and MQA on custom dataset.\relax }}{34}{figure.caption.19}\protected@file@percent }
\newlabel{hg_scaling}{{6.2}{34}{Comparing validation loss and throughput of Hawk, Griffin and MQA on custom dataset.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Character Normalised Accuracy}{34}{subsection.6.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Comparison of character normalized accuracy of different models of different sizes on various downstream tasks.\relax }}{34}{figure.caption.20}\protected@file@percent }
\newlabel{griffin-perf}{{6.3}{34}{Comparison of character normalized accuracy of different models of different sizes on various downstream tasks.\relax }{figure.caption.20}{}}
\abx@aux@cite{0}{gpt2}
\abx@aux@segm{0}{0}{gpt2}
\abx@aux@cite{0}{tinyshakespeare}
\abx@aux@segm{0}{0}{tinyshakespeare}
\abx@aux@cite{0}{maestrov2}
\abx@aux@segm{0}{0}{maestrov2}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Experiments}{35}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Tiny Shakespeare}{35}{section.7.1}\protected@file@percent }
\abx@aux@cite{0}{mnist}
\abx@aux@segm{0}{0}{mnist}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Tiny Shakespeare: Loss Curves\relax }}{36}{figure.caption.21}\protected@file@percent }
\newlabel{ts-loss}{{7.1}{36}{Tiny Shakespeare: Loss Curves\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Generated text samples\relax }}{36}{figure.caption.21}\protected@file@percent }
\newlabel{ts-gen}{{7.2}{36}{Generated text samples\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Maestro V2}{36}{section.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces MIDI: Loss Curves\relax }}{36}{figure.caption.22}\protected@file@percent }
\newlabel{midi-loss}{{7.3}{36}{MIDI: Loss Curves\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Sequential MNIST}{36}{section.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces SMNIST: Loss Curves\relax }}{37}{figure.caption.23}\protected@file@percent }
\newlabel{smnist-loss}{{7.4}{37}{SMNIST: Loss Curves\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Generated digit samples\relax }}{37}{figure.caption.23}\protected@file@percent }
\newlabel{smnist-gen}{{7.5}{37}{Generated digit samples\relax }{figure.caption.23}{}}
\abx@aux@read@bbl@mdfivesum{9B97500F8C4AEC3E4A7BED05F8367404}
\abx@aux@defaultrefcontext{0}{flashattn}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{hawkgriffin}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{h3}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{pileds}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{glorotinit}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{mamba}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{s4}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{hippo}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{dssm}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{maestrov2}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{tinyshakespeare}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{mnist}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{lru}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{universality_lrnn}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{gpt2}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{mqa}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{lra}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{llama2}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{llama}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{transformers}{nty/global//global/global}
\gdef \@abspage@last{43}
